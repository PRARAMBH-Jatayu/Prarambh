{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"cDbOvmrojoer"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJi-0OeJOc0_","outputId":"4fcc56a3-105a-48b1-a025-5481a3de4dc5","executionInfo":{"status":"ok","timestamp":1683435218743,"user_tz":-330,"elapsed":14694,"user":{"displayName":"127_Sarbartha Roy","userId":"09168649608292361059"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers[torch]\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.0+cu118)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (2023.4.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install torch\n","!pip install transformers[torch]"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOMe0Hj_I4aZ","outputId":"541a260d-0729-48f5-f850-5ea200c6786f","executionInfo":{"status":"ok","timestamp":1683435244013,"user_tz":-330,"elapsed":21999,"user":{"displayName":"127_Sarbartha Roy","userId":"09168649608292361059"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEb0bpCCNhal"},"outputs":[],"source":["from torch.utils.data import Dataset\n","import json\n","\n","class ChatData(Dataset):\n","    def __init__(self, path:str, tokenizer):\n","        self.data = json.load(open(path, \"r\"))\n","\n","        self.X = []\n","        for i in self.data:\n","            for j in i['dialog']:\n","                self.X.append(j['text'])\n","\n","        for idx, i in enumerate(self.X):\n","            try:\n","                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n","            except:\n","                break\n","\n","        self.X = self.X[:5000]\n","        \n","        print(self.X[0])\n","\n","        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        self.input_ids = self.X_encoded['input_ids']\n","        self.attention_mask = self.X_encoded['attention_mask']\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return (self.input_ids[idx], self.attention_mask[idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3odQTJUHPAkn","outputId":"c10dbf02-9781-4cdc-94b2-1e6e1aa5f9db","executionInfo":{"status":"ok","timestamp":1683437489435,"user_tz":-330,"elapsed":103979,"user":{"displayName":"127_Sarbartha Roy","userId":"09168649608292361059"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<startofstring> A Miss Frazil on the blower, Pop. <bot>: hi there <endofstring>\n","training .... \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","  8%|▊         | 1/12 [00:08<01:29,  8.10s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <pad> <pad>,,,,,,,,,, <pad>,\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 17%|█▋        | 2/12 [00:16<01:20,  8.00s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 25%|██▌       | 3/12 [00:24<01:12,  8.03s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 33%|███▎      | 4/12 [00:32<01:04,  8.02s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 42%|████▏     | 5/12 [00:40<00:56,  8.02s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <bot>: <bot>: <bot>: <bot>: <bot>: <bot>: <bot>: <bot>: <bot>: <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 50%|█████     | 6/12 [00:48<00:48,  8.04s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 58%|█████▊    | 7/12 [00:56<00:40,  8.04s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: i am not sure? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 67%|██████▋   | 8/12 [01:04<00:32,  8.14s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: i am a huge gamer <bot>: Hi, you are you? <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 75%|███████▌  | 9/12 [01:12<00:24,  8.15s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: Hi, how are you? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 83%|████████▎ | 10/12 [01:22<00:17,  8.58s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: I am a huge gamer. What are doing? <bot>: I am not\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"," 92%|█████████▏| 11/12 [01:30<00:08,  8.48s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: Hi? What are doing? <bot>: I am a dog? <bot>: Hi\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","100%|██████████| 12/12 [01:38<00:00,  8.22s/it]"]},{"output_type":"stream","name":"stdout","text":["<startofstring> hello how are you <bot>: I am not sure what you? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad>\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","#from ChatData import ChatData\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","import tqdm\n","import torch\n","\n","def train(chatData, model, optim):\n","\n","    epochs = 12\n","\n","    for i in tqdm.tqdm(range(epochs)):\n","        for X, a in chatData:\n","            X = X.to(device)\n","            a = a.to(device)\n","            optim.zero_grad()\n","            loss = model(X, attention_mask=a, labels=X).loss\n","            loss.backward()\n","            optim.step()\n","        torch.save(model.state_dict(), \"model_state.pt\")\n","        print(infer(\"hello how are you\"))\n","\n","def infer(inp):\n","    inp = \"<startofstring> \"+inp+\" <bot>: \"\n","    inp = tokenizer(inp, return_tensors=\"pt\")\n","    X = inp[\"input_ids\"].to(device)\n","    a = inp[\"attention_mask\"].to(device)\n","    output = model.generate(X, attention_mask=a )\n","    output = tokenizer.decode(output[0])\n","    return output\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n","                                \"bos_token\": \"<startofstring>\",\n","                                \"eos_token\": \"<endofstring>\"})\n","tokenizer.add_tokens([\"<bot>:\"])\n","\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.resize_token_embeddings(len(tokenizer))\n","\n","model = model.to(device)\n","\n","# print(tokenizer.decode(model.generate(**tokenizer(\"hey i was good at basketball but \",\n","#                          return_tensors=\"pt\"))[0]))\n","\n","chatData = ChatData(\"/content/drive/MyDrive/finetuned-gpt2-convai-main/chat_data.json\", tokenizer)\n","chatData =  DataLoader(chatData, batch_size=64)\n","\n","model.train()\n","\n","optim = Adam(model.parameters(), lr=1e-3)\n","\n","print(\"training .... \")\n","train(chatData, model, optim)\n","\n"]},{"cell_type":"code","source":["# save your trained model to a .pt file\n","torch.save(model.state_dict(), '/content/drive/MyDrive/finetuned-gpt2-convai-main/model.pt' )"],"metadata":{"id":"dtiEd-n0k6ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AwB3ZOl1VgO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function for asking and getting the output\n","print(\"infer from model : \")\n","while True:\n","  inp = input()\n","  print(infer(inp))\n","  if(inp.lower()=='stop'):\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXOc68OW3K7o","outputId":"edb8cb19-efba-4408-b2e8-ebd35087cb8b"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["infer from model : \n","hello\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["<startofstring> hello <bot>: i am a huge gamer, my mom is a very good person. <endofstring> <pad> <pad>\n","stop\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["<startofstring> stop <bot>: I am a very experienced, but it is good to have a good time <endofstring> <pad>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6SX-6pugvtLH"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}